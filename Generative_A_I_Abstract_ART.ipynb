{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN+ZW9DVos+vaipPtZl2pDj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZacariDigital/Generative-AI-Absract-Art/blob/main/Generative_A_I_Abstract_ART.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/openai/CLIP\n",
        "!git clone https://github.com/CompVis/taming-transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWiNVbiTE4bQ",
        "outputId": "3f2aabca-ef34-4a4f-c234-fe10d97a2fe9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'CLIP'...\n",
            "remote: Enumerating objects: 243, done.\u001b[K\n",
            "remote: Total 243 (delta 0), reused 0 (delta 0), pack-reused 243\u001b[K\n",
            "Receiving objects: 100% (243/243), 8.92 MiB | 12.26 MiB/s, done.\n",
            "Resolving deltas: 100% (123/123), done.\n",
            "Cloning into 'taming-transformers'...\n",
            "remote: Enumerating objects: 1339, done.\u001b[K\n",
            "remote: Total 1339 (delta 0), reused 0 (delta 0), pack-reused 1339\u001b[K\n",
            "Receiving objects: 100% (1339/1339), 409.77 MiB | 16.75 MiB/s, done.\n",
            "Resolving deltas: 100% (279/279), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Installing extra libraries\n",
        "!pip install --no-deps ftfy regex tqdm\n",
        "!pip install omegaconf==2.0.0 pytorch-lightning==1.0.8\n",
        "!pip uninstall torchtext --yes\n",
        "!pip install einops\n",
        "!pip install torchvision"
      ],
      "metadata": {
        "id": "WNdJnVQjJPw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch"
      ],
      "metadata": {
        "id": "Xp4_S0HxOCwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##importing libaries\n",
        "import numpy as np\n",
        "import torch, os, imageio, pdb, math\n",
        "import torchvision \n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "import PIL \n",
        "import matplotlib.pyplot as plt \n",
        "\n",
        "import yaml \n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "from CLIP import clip\n",
        "#import warnings\n",
        "#warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "SbB6Di3SK8ye"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Coding Helper functions\n",
        "\n",
        "def show_from_tensor(tensor):\n",
        "  img = tensor.clone()\n",
        "  img = img.mul(255).byte()\n",
        "  img = img.cpu().numpy().transponse((1,2,0))\n",
        "\n",
        "  plt.figure(figsize=(10,7))\n",
        "  plt.axis('off')\n",
        "  plt.imshow(img)\n",
        "  plt.show()\n",
        "\n",
        "def norm_data(data):\n",
        "  return(data.clip(-1,1)+1)/2 ##range between 0 & 1 in the result\n",
        "\n",
        "##Parameters\n",
        "learning_rate = .5\n",
        "batch_size = 1\n",
        "wd = .1\n",
        "noise_factor = .1\n",
        "\n",
        "\n",
        "total_iter=100\n",
        "im_shape = [225, 400, 3] #h,w,channel\n",
        "size1, size2, channels = im_shape\n"
      ],
      "metadata": {
        "id": "1GzqX0c7PbtR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## CLIP MODEL ##\n",
        "clipmodel, _ = clip.load('ViT-B/32', jit=False)\n",
        "clipmodel.eval()\n",
        "print(clip.available_models())\n",
        "\n",
        "print(\"Clip model visual input resolution\", clipmodel.visual.input_resolution)\n",
        "device=torch.device(\"cuda:0\")\n",
        "torch.cuda.empty_cache"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UT8_NmjKRqj8",
        "outputId": "3c802368-1e7a-44b1-8655-fc9311e9c8b5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 338M/338M [00:02<00:00, 130MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px']\n",
            "Clip model visual input resolution 224\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function torch.cuda.memory.empty_cache() -> None>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Taming Transformer##\n",
        "\n",
        "%cd taming-transformers/\n",
        "\n",
        "!mkdir -p models/vqgan_imagenet_f16_16384/checkpoints\n",
        "!mkdir -p models/vqgan_imagenet_f16_16384/configs\n",
        "\n",
        "if len(os.listdir('models/vqgan_imagenet_f16_16384/configs/')) == 0:\n",
        "  !wget 'https://heibox.uni-heidelberg.de/f/867b05fc8c4841768640/?dl=1' -O 'models/vqgan_imagenet_f16_16384/checkpoints/last.ckpt'\n",
        "  !wget 'https://heibox.uni-heidelberg.de/f/274fb24ed38341bfa753/?dl=1' -O 'models/vqgan_imagenet_f16_16384/configs/model.yaml'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gf4mF_qJS2_g",
        "outputId": "631c4b95-2436-4723-cbcb-40bda981f3f6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'taming-transformers/'\n",
            "/content/taming-transformers\n",
            "--2023-05-07 22:26:17--  https://heibox.uni-heidelberg.de/f/867b05fc8c4841768640/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/f85cf3cc-7219-46e3-a5ba-78927a5f7dde/last.ckpt [following]\n",
            "--2023-05-07 22:26:18--  https://heibox.uni-heidelberg.de/seafhttp/files/f85cf3cc-7219-46e3-a5ba-78927a5f7dde/last.ckpt\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 980092370 (935M) [application/octet-stream]\n",
            "Saving to: ‘models/vqgan_imagenet_f16_16384/checkpoints/last.ckpt’\n",
            "\n",
            "models/vqgan_imagen 100%[===================>] 934.69M  15.3MB/s    in 64s     \n",
            "\n",
            "2023-05-07 22:27:22 (14.7 MB/s) - ‘models/vqgan_imagenet_f16_16384/checkpoints/last.ckpt’ saved [980092370/980092370]\n",
            "\n",
            "--2023-05-07 22:27:22--  https://heibox.uni-heidelberg.de/f/274fb24ed38341bfa753/?dl=1\n",
            "Resolving heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)... 129.206.7.113\n",
            "Connecting to heibox.uni-heidelberg.de (heibox.uni-heidelberg.de)|129.206.7.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://heibox.uni-heidelberg.de/seafhttp/files/ad102671-54bb-425d-91a7-09e72ab598eb/model.yaml [following]\n",
            "--2023-05-07 22:27:23--  https://heibox.uni-heidelberg.de/seafhttp/files/ad102671-54bb-425d-91a7-09e72ab598eb/model.yaml\n",
            "Reusing existing connection to heibox.uni-heidelberg.de:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 692 [application/octet-stream]\n",
            "Saving to: ‘models/vqgan_imagenet_f16_16384/configs/model.yaml’\n",
            "\n",
            "models/vqgan_imagen 100%[===================>]     692  --.-KB/s    in 0.007s  \n",
            "\n",
            "2023-05-07 22:27:24 (96.5 KB/s) - ‘models/vqgan_imagenet_f16_16384/configs/model.yaml’ saved [692/692]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}